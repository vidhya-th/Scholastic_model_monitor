import sys
import os
from dataclasses import dataclass
import numpy as np
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline   
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from src.exception import CustomException
from src.logger import logging
from src.utils import save_object


from src.utils import save_object

# Configuration class to define where the serialized preprocessor will be saved
@dataclass
class DataTransformationConfig:
    # Path to save the trained preprocessor object for later use in prediction pipelines
    preprocessor_obj_file_path = os.path.join('artifacts', 'preprocessor.pkl')

class DataTransformation:
    def __init__(self):
        # Initialize configuration to access file paths
        self.data_transformation_config = DataTransformationConfig()

    def get_data_transformer_object(self):
        """
        Creates and returns a ColumnTransformer object for data preprocessing.
        """
        try:
            # Define feature groups for targeted transformations
            numerical_columns = ['writing_score', 'reading_score']
            categorical_columns = [
                'gender', 
                'race_ethnicity', 
                'parental_level_of_education', 
                'lunch', 
                'test_preparation_course'
            ]
            
            # Numerical Pipeline: Handles missing values then scales data
            # SimpleImputer ensures no null values; StandardScaler normalizes features (mean=0, std=1)
            num_pipeline = Pipeline(
                steps=[
                    ('imputer', SimpleImputer(strategy='median')),
                    ('scaler', StandardScaler())
                ]
            )
            
            # Categorical Pipeline: Handles missing values, encodes categories, then scales
            # OneHotEncoder converts strings to binary vectors
            # with_mean=False is used in StandardScaler for sparse matrices generated by OneHotEncoder
            cat_pipeline = Pipeline(
                steps=[
                    ('imputer', SimpleImputer(strategy='most_frequent')),
                    ('one_hot_encoder', OneHotEncoder()),
                    ('scaler', StandardScaler(with_mean=False))
                ]
            )
            
            logging.info(f"Numerical columns: {numerical_columns}")
            logging.info(f"Categorical columns: {categorical_columns}")
            
            # ColumnTransformer: Merges the two pipelines into a single preprocessing unit
            # It applies num_pipeline to numerical_columns and cat_pipeline to categorical_columns
            preprocessor = ColumnTransformer(
                transformers=[
                    ('num_pipeline', num_pipeline, numerical_columns),
                    ('cat_pipeline', cat_pipeline, categorical_columns)
                ]
            ) 

            return preprocessor
        
        except Exception as e:
            # Standard error handling for MLOps traceability
            logging.error("Error in get_data_transformer_object")
            raise CustomException(e, sys)
        
    def initiate_data_transformation(self, train_path, test_path):
        """
        Initiates the data transformation process on training and testing datasets.
        """
        try:
            # Load datasets
            train_df = pd.read_csv(train_path)
            test_df = pd.read_csv(test_path)
            logging.info("Read train and test data completed")
            
            logging.info("Obtaining preprocessing object")
            preprocessing_obj = self.get_data_transformer_object()
            
            # Define target and feature columns
            target_column_name = 'math_score'
            numerical_columns = ['writing_score', 'reading_score']
            
            # Separate features and target in training data
            input_feature_train_df = train_df.drop(columns=[target_column_name], axis=1)
            target_feature_train_df = train_df[target_column_name]
            
            # Separate features and target in testing data
            input_feature_test_df = test_df.drop(columns=[target_column_name], axis=1)
            target_feature_test_df = test_df[target_column_name]
            
            # Apply transformations to training data
            input_feature_train_arr = preprocessing_obj.fit_transform(input_feature_train_df)
            train_arr = np.c_[input_feature_train_arr, np.array(target_feature_train_df)]
            
            # Apply transformations to testing data
            input_feature_test_arr = preprocessing_obj.transform(input_feature_test_df)
            test_arr = np.c_[input_feature_test_arr, np.array(target_feature_test_df)]
            
            logging.info("Applied preprocessing object on training and testing datasets")
            
            # Save the preprocessor object for future use
            
            save_object(
                file_path=self.data_transformation_config.preprocessor_obj_file_path,
                obj=preprocessing_obj
            )
            logging.info("Preprocessor object saved")

            return train_arr, test_arr, self.data_transformation_config.preprocessor_obj_file_path
        
        except Exception as e:
            logging.error("Error in initiate_data_transformation")
            raise CustomException(e, sys)
